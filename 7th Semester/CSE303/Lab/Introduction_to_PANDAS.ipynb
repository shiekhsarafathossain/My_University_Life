{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO6xyk_oJXYB"
      },
      "source": [
        "# **Introduction to Pandas** #\n",
        "**Pandas** is an open-source Python library primarily used for **data manipulation and analysis**. It provides data structures and functions that make it easy to work with structured data, particularly **data in the form of tables**.  \n",
        "Pandas is widely used in data science, machine learning, and data analysis because of its powerful and easy-to-use data structures, Series and DataFrame.\n",
        "\n",
        "\n",
        "###**Key Data Structures in Pandas:**###\n",
        "\n",
        "1. **Series:** A one-dimensional array-like object containing a sequence of values.\n",
        "Each value is associated with an index label.\n",
        "2. **DataFrame:** A two-dimensional labeled data structure with columns that can hold different data types.\n",
        "\n",
        "**Common Data Analysis Tasks with Pandas:**\n",
        "\n",
        "1. **Data Import/Export:** Reading data from various file formats (CSV, Excel, JSON, etc.) and writing data to different formats.\n",
        "2. **Data Cleaning:** Handling missing values, removing duplicates, and correcting inconsistencies.\n",
        "3. **Data Manipulation:** Filtering, sorting, grouping, and transforming data.\n",
        "4. **Data Analysis:** Statistical calculations, time series analysis, and exploratory data analysis.\n",
        "5. **Data Visualization:** Creating informative visualizations using libraries like Matplotlib and Seaborn.\n",
        "\n",
        "**Installing Pandas:**\n",
        "To use pandas, you need to install it using pip:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "pip install pandas\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEDMFKqkL6KR"
      },
      "source": [
        "##**Basic Examples**##\n",
        "1. **Importing Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ckEJ5QpJT7U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAAkBD4wMGBe"
      },
      "source": [
        "2. **Creating a Series**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XYCOoytLMK_N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    2\n",
            "2    3\n",
            "3    4\n",
            "4    5\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Creating a simple Series\n",
        "data = pd.Series([1, 2, 3, 4, 5])\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f48r9v_uMQkr"
      },
      "source": [
        "3. **Creating a DataFrame**\n",
        "The DataFrame is like a table with rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EHyEU9SqMUbg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Name  Age           City\n",
            "0    Alice   24       New York\n",
            "1      Bob   27  San Francisco\n",
            "2  Charlie   22    Los Angeles\n"
          ]
        }
      ],
      "source": [
        "# Creating a DataFrame from a dictionary\n",
        "data = {\n",
        "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'Age': [24, 27, 22],\n",
        "    'City': ['New York', 'San Francisco', 'Los Angeles']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahHeHHxcMaUW"
      },
      "source": [
        "4. **Reading a CSV File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jU5DIDZ8MeFt"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
            "File \u001b[1;32md:\\Software\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32md:\\Software\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32md:\\Software\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[1;32md:\\Software\\Anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32md:\\Software\\Anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"data.csv\")\n",
        "print(df.head())  # Display the first few rows of the DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVwLHaQMhzU"
      },
      "source": [
        "5. **Data Filtering and Manipulation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njEDW8XdMk00"
      },
      "outputs": [],
      "source": [
        "# Selecting rows where age > 23\n",
        "filtered_df = df[df['Age'] > 23]\n",
        "print(filtered_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3dvhj0E0pDH"
      },
      "source": [
        "## READING OUR TEST FILE ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYwE-6HG0tEi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "file_name = \"Dataset_Lab_Test.csv\"  # File name\n",
        "data = pd.read_csv(file_name)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmNWl4vA03JE"
      },
      "source": [
        "If the dataset has no header, use the header=None parameter and assign column names manually:\n",
        "\n",
        "```\n",
        "data = pd.read_csv(file_name, header=None, names=['Year', 'Month', 'Region', 'Product_A', 'Product_B', 'Product_C'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfEe-Wg31jUW"
      },
      "source": [
        "### Calculate MEAN, STANDARD DEVIATION, VARIANCE, MEDIAN, MODE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlD1h3_a1gJ6"
      },
      "outputs": [],
      "source": [
        "# Calculate statistics for Product_A\n",
        "average = data['Product_A'].mean()\n",
        "std_dev = data['Product_A'].std()\n",
        "variance = data['Product_A'].var()\n",
        "median = data['Product_A'].median()\n",
        "mode = data['Product_A'].mode()\n",
        "\n",
        "# Display the results\n",
        "print(f\"Average (Mean) of Product_A: {average}\")\n",
        "print(f\"Standard Deviation of Product_A: {std_dev}\")\n",
        "print(f\"Variance of Product_A: {variance}\")\n",
        "print(f\"Median of Product_A: {median}\")\n",
        "print(f\"Mode of Product_A: {mode.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDNFOp5r2HTq"
      },
      "source": [
        "###**Calculate MEAN grouped by something (year or region)**###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRWsCKTL0vSX"
      },
      "outputs": [],
      "source": [
        "# Group by 'Year' and calculate the average of 'Product_A'\n",
        "average_product_a_by_year = data.groupby('Year')['Product_A'].mean()\n",
        "\n",
        "# Display the result\n",
        "print(average_product_a_by_year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPf9jXKO2yyG"
      },
      "outputs": [],
      "source": [
        "# Group by 'Region' and calculate the mean for each product\n",
        "mean_by_region = data.groupby('Region')[['Product_A', 'Product_B', 'Product_C']].mean()\n",
        "\n",
        "# Display the result\n",
        "print(mean_by_region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obp9m3gV2tBC"
      },
      "source": [
        "## DATA CLEANING WITH PANDAS ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WToLr8Ig4AYq"
      },
      "source": [
        "Data cleaning is a crucial part of data preprocessing to ensure your dataset is free from errors, inconsistencies, or irrelevant data. Pandas provides powerful tools for cleaning and preparing data for analysis. Below are common data cleaning tasks and how to perform them with Pandas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fzs8DP64Czd"
      },
      "source": [
        "###**1. Handling Missing Data**###\n",
        "\n",
        "**Check for missing values:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohkdK7Hr3-0t"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in each column\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Display rows with missing data\n",
        "print(data[data.isnull().any(axis=1)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_xTj3oc4YUq"
      },
      "source": [
        "**Drop rows or columns with missing values:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASQOaKaS4dcq"
      },
      "outputs": [],
      "source": [
        "data = data.dropna()  # Removes rows with any missing values\n",
        "data = data.dropna(axis=1)  # Removes columns with any missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uadLloaP5Xbm"
      },
      "source": [
        "**Filling missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjvN-E-l5eiO"
      },
      "outputs": [],
      "source": [
        "# Fill missing values with a specific value\n",
        "data['Column_Name'] = data['Column_Name'].fillna(0)\n",
        "\n",
        "# Fill with the mean/median/mode\n",
        "data['Column_Name'] = data['Column_Name'].fillna(data['Column_Name'].mean())\n",
        "data['Column_Name'] = data['Column_Name'].fillna(data['Column_Name'].median())\n",
        "data['Column_Name'] = data['Column_Name'].fillna(data['Column_Name'].mode()[0])\n",
        "\n",
        "# Forward/Backward fill\n",
        "data['Column_Name'] = data['Column_Name'].fillna(method='ffill')  # Forward fill\n",
        "data['Column_Name'] = data['Column_Name'].fillna(method='bfill')  # Backward fill\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRjcdvSR4hMY"
      },
      "source": [
        "###**2. Removing Duplicate Rows**###\n",
        "\n",
        "**`Find duplicates:`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBVjyz3G4r7j"
      },
      "outputs": [],
      "source": [
        "print(data.duplicated())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDbmOXWI4s9Z"
      },
      "source": [
        "**Remove duplicates:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClLYGpZ54yYL"
      },
      "outputs": [],
      "source": [
        "data = data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at2_bXkp5qSG"
      },
      "source": [
        "###**3. Removing Outliers**###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0yqUSXM5uNK"
      },
      "outputs": [],
      "source": [
        "# Identify outliers using the IQR method\n",
        "Q1 = data['Product_C'].quantile(0.25)\n",
        "Q3 = data['Product_C'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the lower and upper bounds\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# Filter out rows with outliers\n",
        "data = data[(data['Product_C'] >= lower_bound) & (data['Product_C'] <= upper_bound)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRRDa3WD6V7W"
      },
      "source": [
        "###**4. Convert Data Types**###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GluH7uZ96Tpc"
      },
      "outputs": [],
      "source": [
        "# Correct data types\n",
        "data['Year'] = data['Year'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgTeO_v86ZxU"
      },
      "source": [
        "###**5. Normalize Product A**###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aRxFfvC6mJ6"
      },
      "outputs": [],
      "source": [
        "# Normalize Product_A\n",
        "data['Normalized_Product_A'] = (data['Product_A'] - data['Product_A'].min()) / (data['Product_A'].max() - data['Product_A'].min())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfYIzTr161JK"
      },
      "source": [
        "###**6. Filter Invalid Inputs**###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icqO3qvZ6zPJ"
      },
      "outputs": [],
      "source": [
        "# Remove rows where a column value does not meet a condition\n",
        "data = data[data['Year'] > 2021]  # Keep rows where values are greater than 0\n",
        "data = data[data['Region'].isin(['North', 'South', 'East', 'West'])]  # Keep rows with specific categories\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
